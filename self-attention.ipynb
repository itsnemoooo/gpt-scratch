{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 #batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: Averaging with For Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "cons: computationally intensive\n",
    "\n",
    "\n",
    "pros: easy to understand, each character will have the context of the previous tokens in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the goal is x[b,t] = x[b,t-1] + x[b,t-2] + x[b,t-3] + x[b,t-4]\n",
    "#we can do this with a for loop\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #all the previous tokens in the sequence\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0) #average the previous tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Matrix Multiplication as Weighted Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n",
      "c=\n",
      "tensor([[17., 12.],\n",
      "        [17., 12.],\n",
      "        [17., 12.]])\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#using tril to get the lower triangular part of the matrix\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "print('a=')\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n",
      "c=\n",
      "tensor([[8.0000, 6.0000],\n",
      "        [6.5000, 4.0000],\n",
      "        [5.6667, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "#depending on how many 1s and zeros we have, we are doing a sum of the rows and depositing into c\n",
    "#we can do incremnetal average update to c as well\n",
    "\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "print('a='  )\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "#now we can do the weighted aggregation\n",
    "c = a @ b\n",
    "print('c=')\n",
    "print(c)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look how the first row of c is the same as the first row of b\n",
    "#this is because we are averaging the previous tokens\n",
    "#our a matrix defines the averaging scheme\n",
    "\n",
    "#now we can do the same thing with the transformer\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x #this creates (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "torch.allclose(xbow, xbow2)\n",
    "\n",
    "#this is a much more efficient way to do the weighted aggregation\n",
    "#we are using matrix multiplication to do the weighted aggregation\n",
    "#this is the same as the for loop, but it is much more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]\n",
    "#they are identical. we used batch matrix multiplication to do this weighted aggregation (sepcified in the T x T array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3: Using Soft-max\n",
    "\n",
    "1. These weights begin with 0 but this will not always be the case. How much of each token from the past do we want to aggregate?\n",
    "\n",
    "These tokens will start looking at each other. They will have affinities.\n",
    "\n",
    "2. The past still cannot communicate with the future.\n",
    "\n",
    "3. Normalise and..\n",
    "\n",
    "4. Sum to aggregate values depending on how interesting they find each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril  = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T, T)) #1. \n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #2. for all the elements where tril is 0 make them -inf\n",
    "wei = F.softmax(wei, dim=-1) #3. on every row do a softmax\n",
    "xbow3 = wei @ x #4. multiply the weights by the values\n",
    "torch.allclose(xbow, xbow3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4: Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "#single head self attention. we want the wei to be data-dependent\n",
    "B, T, C = 4, 8, 32 #batch, time, channels (from 2 to 32 now)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# query (what am I looking for), key (what am I looking at)\n",
    "# we want to compute the attention between the query and the key. if the key and query are similar they will have a high attention score\n",
    "#wei = torch.zeros((T, T)) #1. single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) #(B, T, 16) -> (B, T, head_size)\n",
    "q = query(x) #(B, T, 16) -> (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) #(B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "#for every row of B we will a T x T matrix of the affinities between the query and the key\n",
    "tril  = torch.tril(torch.ones(T,T))\n",
    "\n",
    "#wei = wei.masked_fill(tril == 0, float('-inf')) #2. for all the elements where tril is 0 make them -inf\n",
    "#wei = F.softmax(wei, dim=-1) #3. on every row do a softmax\n",
    "out = wei @ x #4. multiply the weights by the values\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the raw wei matrix, we can see that the attention is being computed between the query and the key\n",
    "#the wei matrix is now data-dependent.\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can now mask the wei matrix so that the future cannot communicate with the past\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "#wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ x\n",
    "\n",
    "wei[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4592, 0.5408, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3280, 0.3267, 0.3452, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2559, 0.2477, 0.2487, 0.2477, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1983, 0.1988, 0.1984, 0.1983, 0.2061, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1659, 0.1670, 0.1659, 0.1658, 0.1697, 0.1658, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1437, 0.1426, 0.1426, 0.1427, 0.1430, 0.1425, 0.0000],\n",
       "        [0.1248, 0.1249, 0.1249, 0.1252, 0.1249, 0.1249, 0.1252, 0.1252]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and apply a softmax to the wei matrix\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "wei[0]\n",
    "\n",
    "#This tells us how much information to aggregate from the past.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we don't aggregate the raw x we create a new representation of x called 'v' - single head aggregation between nodes\n",
    "#repeat code:\n",
    "torch.manual_seed(1337)\n",
    "#single head self attention. we want the wei to be data-dependent\n",
    "B, T, C = 4, 8, 32 #batch, time, channels (from 2 to 32 now)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# query (what am I looking for), key (what am I looking at)\n",
    "# we want to compute the attention between the query and the key. if the key and query are similar they will have a high attention score\n",
    "#wei = torch.zeros((T, T)) #1. single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) #(B, T, 16) -> (B, T, head_size)\n",
    "q = query(x) #(B, T, 16) -> (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) #(B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "#for every row of B we will a T x T matrix of the affinities between the query and the key\n",
    "tril  = torch.tril(torch.ones(T,T))\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #2. for all the elements where tril is 0 make them -inf\n",
    "wei = F.softmax(wei, dim=-1) #3. on every row do a softmax\n",
    "\n",
    "v = value(x) #(B, T, 16) -> (B, T, head_size)\n",
    "out = wei @ v #4. multiply the weights by the values\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
